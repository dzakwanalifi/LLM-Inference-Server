# ==============================================================================
# CONFIGURASI ENVIRONMENT VARIABLES UNTUK LLM INFERENCE SERVER
# ==============================================================================
# 
# INSTRUKSI:
# 1. Copy file ini menjadi .env
# 2. Ganti nilai-nilai sesuai dengan konfigurasi server Anda
# 3. JANGAN commit file .env ke repository (sudah di-ignore)
#
# ==============================================================================

# ==============================================================================
# API KEY & KEAMANAN
# ==============================================================================

# API Key untuk autentikasi request ke server
# Ganti dengan kunci rahasia yang kuat dan unik
# Contoh: API_KEY="sk-1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef"
API_KEY="ganti_dengan_kunci_rahasia_yang_kuat_dan_unik"

# ==============================================================================
# MODEL CONFIGURATION
# ==============================================================================

# Checksum SHA256 dari file model untuk verifikasi integritas
# Opsional: untuk memastikan model tidak corrupt
# Contoh: MODEL_CHECKSUM="07e4917a026e6f9b69992a5433d9f37c376174a2ff4389658f696e57285227ec"
MODEL_CHECKSUM="07e4917a026e6f9b69992a5433d9f37c376174a2ff4389658f696e57285227ec"

# ==============================================================================
# SERVER CONFIGURATION (Opsional - bisa di-override di docker-compose)
# ==============================================================================

# Port yang digunakan server (default: 8000)
# SERVER_PORT=8000

# Host binding (default: 0.0.0.0 untuk semua interface)
# SERVER_HOST="0.0.0.0"

# Log level (default: INFO)
# LOG_LEVEL="INFO"

# ==============================================================================
# MODEL PERFORMANCE TUNING (Opsional)
# ==============================================================================

# Jumlah thread CPU untuk inference (sesuaikan dengan core CPU Anda)
# MODEL_N_THREADS=4

# Context window size untuk model (default: 4096)
# MODEL_N_CTX=4096

# Batch size untuk pemrosesan prompt (default: 1024)
# MODEL_N_BATCH=1024

# ==============================================================================
# RATE LIMITING (Opsional)
# ==============================================================================

# Rate limit untuk API (default: 15 request per menit per IP)
# RATE_LIMIT="15/minute"

# ==============================================================================
# HEALTH CHECK (Opsional)
# ==============================================================================

# Interval health check dalam detik (default: 10)
# HEALTH_CHECK_INTERVAL=10

# Timeout untuk dummy inference dalam detik (default: 5)
# HEALTH_CHECK_TIMEOUT=5

# ==============================================================================
# LOGGING (Opsional)
# ==============================================================================

# Format log (default: JSON)
# LOG_FORMAT="json"

# Output log (default: console)
# LOG_OUTPUT="console"

# ==============================================================================
# DOCKER SPECIFIC (Opsional)
# ==============================================================================

# User ID untuk menjalankan container (default: 1000)
# DOCKER_UID=1000

# Group ID untuk menjalankan container (default: 1000)
# DOCKER_GID=1000

# ==============================================================================
# NOTES:
# ==============================================================================
# 
# 1. Semua nilai yang dimulai dengan # adalah komentar dan tidak akan diproses
# 2. Gunakan tanda kutip untuk nilai yang mengandung spasi atau karakter khusus
# 3. Jangan ada spasi sebelum dan sesudah tanda =
# 4. Restart server setelah mengubah environment variables
# 5. Untuk production, gunakan secrets management yang proper
#
# ==============================================================================
